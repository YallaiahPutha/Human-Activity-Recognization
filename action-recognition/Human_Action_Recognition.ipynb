{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85319d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Downloads/Dataset/'\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f6c78",
   "metadata": {},
   "source": [
    "# Testing For Getting images or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f494cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot with specific imaze size\n",
    "plt.figure(figsize=(20,20))\n",
    "# Getting class names from Dataset\n",
    "all_class_names_list = os.listdir(dataset)\n",
    "#Generate the random images from 3 classes\n",
    "random_range = random.sample(range(len(all_class_names_list)),3)\n",
    "\n",
    "# Iterating through all the values in random_range\n",
    "for counter, random_index in enumerate(random_range, 1):\n",
    "    \n",
    "    #retrive a class name based on the index\n",
    "    selected_class_names = all_class_names_list[random_index]\n",
    "    \n",
    "    # retrive the list of all video files present in randomly seleted class_directory\n",
    "    video_files_names_list = os.listdir(f'Downloads/Dataset/{selected_class_names}')\n",
    "    \n",
    "    # Select the video files randomly \n",
    "    selected_video_file_names = random.choice(video_files_names_list)\n",
    "    \n",
    "    # Caputure the object from videos\n",
    "    video_reader = cv2.VideoCapture(f'Downloads/Dataset/{selected_class_names}/{selected_video_file_names}')\n",
    "    \n",
    "    # Read the frames from video files\n",
    "    _, bgr_frame = video_reader.read()\n",
    "    \n",
    "    # Release the video capture object\n",
    "    video_reader.release()\n",
    "    \n",
    "    # Convert the frames into RGB format\n",
    "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Put text on images based on the label name\n",
    "    cv2.putText(rgb_frame, selected_class_names,(10,30), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    \n",
    "    # Display frames\n",
    "    plt.subplot(5,4, counter);\n",
    "    plt.imshow(rgb_frame);\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b85721",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7458ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Directory\n",
    "DATASET_DIR = \"./Downloads/Dataset/\"\n",
    "\n",
    "# Image size of each frame will resize\n",
    "Image_height,Image_width = 224, 224\n",
    "\n",
    "# Specify the number of frames fed to the  model\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "\n",
    "\n",
    "# Specify the Classes list\n",
    "CLASSES_LIST = [\"Falling\",\"Loitering\",\"Voilence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305869d8",
   "metadata": {},
   "source": [
    "# Function to Extract the frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    \n",
    "    # Declare the list to store the video frames\n",
    "    frames_list = []\n",
    "    \n",
    "    # Videocapture from the video files\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    # Total number of Frames Count\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # skipping some of frames based on the action intervals\n",
    "    skip_frame_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "    \n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        # Current Frame Position of the video\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter*skip_frame_window)\n",
    "        #Read the frames\n",
    "        success,frames = video_reader.read()\n",
    "        # checking the frames successfully read or not in case not skip\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        # Resize the frames with fixed height and width\n",
    "        resized_frames = cv2.resize(frames, (Image_height,Image_width))\n",
    "        \n",
    "        # Normalize the image\n",
    "        normalized_frames = resized_frames/255\n",
    "        \n",
    "        # Append the normalized images to the frameslist\n",
    "        frames_list.append(normalized_frames)\n",
    "    \n",
    "    # video object capture release\n",
    "    video_reader.release()\n",
    "    \n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5d7ea",
   "metadata": {},
   "source": [
    "# Create A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    features = []\n",
    "    labels  = []\n",
    "    video_files_paths =  []\n",
    "    \n",
    "    # Iterating through all the classes mentioned the classes list\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        print(f'Extracting data of class: {class_name}')\n",
    "        \n",
    "        # Get the list of video files present in the specific class name\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        print(files_list)\n",
    "        \n",
    "        # Iterate through all the files present\n",
    "        for file_name in files_list:\n",
    "            \n",
    "            # Get complete video path\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "            \n",
    "            # Extract the frames of the video file path\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            \n",
    "            # Check if the extracted frames are equal or not\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                \n",
    "                # Append the features to frames and labels\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "    \n",
    "    # converting the features and labels are into array form \n",
    "    features = np.array(features)\n",
    "    labels   = np.array(labels)\n",
    "        \n",
    "    return features, labels, video_files_paths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lables based on the class each folder in the form of 1 and 0\n",
    "one_hot_encoded_labels = to_categorical(labels)\n",
    "one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features and labels into X_train,X_test,y_train and y_test \n",
    "X_train,X_test,y_train,y_test = train_test_split(features,one_hot_encoded_labels,test_size=.20,shuffle=True,random_state=seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f538621",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b593a",
   "metadata": {},
   "source": [
    "# Model LRCN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model with less layers \n",
    "def LRCN_Model():\n",
    "    \n",
    "    # we use sequential model for model constructions\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Model Architecture\n",
    "    model.add(TimeDistributed(Conv2D(16,(3,3),padding ='same',activation='relu'), input_shape= (SEQUENCE_LENGTH,\n",
    "                                                                               Image_height,Image_width,3)))\n",
    "    # adding maxpooling layers to the model to extract specific data from feature maps\n",
    "    model.add(TimeDistributed(MaxPooling2D(4,4)))\n",
    "    # adding dropout layers to reduce the overfitting problem\n",
    "    model.add(TimeDistributed(Dropout(0.20)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32,(3,3),padding ='same',activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(4,4)))\n",
    "    model.add(TimeDistributed(Dropout(0.20)))\n",
    "    \n",
    "\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64,(3,3),padding ='same',activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "    model.add(TimeDistributed(Dropout(0.20)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3,3), padding ='same', activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "    \n",
    "    # Flattened layer takes all outputs from all convoluation layers to single 1*1 layer \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(32))\n",
    "    \n",
    "    # fully connected layer we are the output based on number of classes\n",
    "    model.add(Dense(len(CLASSES_LIST), activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f05a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcn_model = LRCN_Model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861891d9",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Compile the model and specify loss function\n",
    "\n",
    "lrcn_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model using fit\n",
    "\n",
    "lrcn_model_training = lrcn_model.fit(X_train,y_train,epochs=50, batch_size=4,\n",
    "                                    shuffle=True, validation_split=0.20,\n",
    "                                    callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a32a86",
   "metadata": {},
   "source": [
    "# Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "\n",
    "model_evaluate_history = lrcn_model.evaluate(X_test, y_test)\n",
    "model_evaluate_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec41b7b",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and accuracy for model\n",
    "model_evaluation_loss, model_evaluation_accuracy = model_evaluate_history\n",
    "\n",
    "date_time_format = \"%Y_%m_%d_%H_%M_%S\"\n",
    "current_date_time_dt = dt.datetime.now()\n",
    "current_date_time_string = dt.datetime.strftime(current_date_time_dt,date_time_format)\n",
    "\n",
    "#model file name\n",
    "model_file_name = f'./Models_Save/lrcn_model_Date_Time_{current_date_time_string}_Loss_{model_evaluation_loss}_Accuracy_{model_evaluation_accuracy}_normal1__'\n",
    "\n",
    "#save model \n",
    "lrcn_model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8cd82",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the save model\n",
    "from keras.models import load_model\n",
    "restored_model = load_model('tensorflow_S/Save_Model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8048715",
   "metadata": {},
   "source": [
    "# Getting Output from entire video as Single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b90c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lrcn_model\n",
    "def prediction_video(input_video_path, SEQUENCE_LENGTH):\n",
    "    \n",
    "    frame_list = []\n",
    "    # Capturing the video from the input\n",
    "    video_reader = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Getting Shape of image like Height and Width\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Video frame count\n",
    "    video_frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    skip_frame_window = max(int(video_frame_count/SEQUENCE_LENGTH), 1)\n",
    "    \n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        \n",
    "        # Current Frame Position of the video\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter*skip_frame_window)\n",
    "        #Read the frames\n",
    "        success,frames = video_reader.read()\n",
    "        # checking the frames successfully read or not in case not skip\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        # Resize the frames with fixed height and width\n",
    "        resized_frames = cv2.resize(frames, (Image_height,Image_width))\n",
    "        \n",
    "        # Normalize the image\n",
    "        normalized_frames = resized_frames/255\n",
    "        \n",
    "        # Append the normalized images to the frameslist\n",
    "        frame_list.append(normalized_frames)\n",
    "    \n",
    "    # Getting the probabilites of each classes for image as output\n",
    "    predict_label_prob = restored_model.predict(np.expand_dims(frame_list, axis=0))[0]\n",
    "    # Based on the probablities we used highest probalities as predicted value and their position too\n",
    "    predict_label = np.argmax(predict_label_prob)\n",
    "    # mapped that position index to given classes_list we get the output as lable name\n",
    "    predicted_class_name = CLASSES_LIST[predict_label]\n",
    "    \n",
    "    #Dispaly the results\n",
    "    print(f'Action Prediction:{predicted_class_name}\\nConfidence :{predict_label_prob[predict_label]}')\n",
    "    \n",
    "    video_reader.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = \"Downloads/testdata/videoplayback (online-video-cutter.com) (1).mp4\"\n",
    "# input_video_path = \"Downloads/pexels-mary-taylor-6002473.mp4\"\n",
    "prediction_video(input_video_path,SEQUENCE_LENGTH)\n",
    "VideoFileClip(input_video_path,audio=False,target_resolution=(300,None)).ipython_display(maxduration=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d82d78",
   "metadata": {},
   "source": [
    "# Getting Multiple Outputs from entire Video Based on the Frame by Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def predict_on_video(video_file_path,SEQUENCE_LENGTH):\n",
    "    \n",
    "    # Capturing the video from the input\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # Getting the frames and make as a queue with sequence length\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "    predicted_class_name = ''\n",
    "    list_prediction = []\n",
    "    # we are checking the condition for video\n",
    "    while video_reader.isOpened():\n",
    "        # Read the video frames \n",
    "        ok,frame = video_reader.read()\n",
    "        # checking that the frames are read or not. In case not we will break that\n",
    "        if not ok:\n",
    "            break\n",
    "        \n",
    "        # we are resizing the extracted frames into 224*224\n",
    "        resized_frame = cv2.resize(frame, (Image_height,Image_width))\n",
    "        # we are normalizing the frame in between 0 to 1\n",
    "        normalized_frame = resized_frame / 255\n",
    "        # Appeneding all normalize frames to the one list\n",
    "        frames_queue.append(normalized_frame)\n",
    "        # here we are feeding frames based on the SEQUENCE_LENGTH\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH :\n",
    "            # We are using our trained model for extract the probablities for each class\n",
    "            predicted_labels_probabilities = restored_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
    "            # we are getting the lable index based on the probablites which one is high\n",
    "            predict_label = np.argmax(predicted_labels_probabilities)\n",
    "            # Based on the label index we will getting the class name \n",
    "            predicted_class_name = CLASSES_LIST[predict_label]\n",
    "#            append all lables values for each frame to alist\n",
    "            list_prediction.append(predicted_class_name)\n",
    " \n",
    "    video_reader.release()\n",
    "    return list_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = 'Downloads/fi034_UjEJAleb.mp4'\n",
    "result = predict_on_video(input_video_path, SEQUENCE_LENGTH)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c79de",
   "metadata": {},
   "source": [
    "# Convert the Deep Learning Model into TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640031c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "saved_model_dir = \"tensorflow_S/Save_Model/\"\n",
    "output_model_dir = 'tensorflow_S/model_to22.tflite'\n",
    "def normalmodel_to_tflitemodel(saved_model_dir,output_model_dir):\n",
    "    # here we are loading our saved deep learning model\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, signature_keys=['serving_default'])\n",
    "    # In optimization we reduce the size and make same accuracy\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "    # converting into the tflite model\n",
    "    tflite_model = converter.convert()\n",
    "    # saving in specific path \n",
    "    with open(output_model_dir, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de99b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalmodel_to_tflitemodel(saved_model_dir,output_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4e4ab",
   "metadata": {},
   "source": [
    "# Prediction with TFLite model for Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0301e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.lite as tflite\n",
    "import time\n",
    "from collections import deque\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tflite.Interpreter(model_path='tensorflow_S/model_to1.tflite')\n",
    "#allocate the tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Checking the input and output details from model\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Specify the number of frames fed to the  model\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "# Image size of each frame will resize\n",
    "Image_height,Image_width = 224, 224\n",
    "\n",
    "# Specify the Classes list\n",
    "CLASSES_LIST = [\"Falling\",\"Loitering\",\"Voilence\"]\n",
    "\n",
    "\n",
    "def tflite_model_prediction(input_video_path, SEQUENCE_LENGTH):\n",
    "    \n",
    "    #     start = time.time()\n",
    "    # Capturing the video from the input\n",
    "    video_reader = cv2.VideoCapture(input_video_path)\n",
    "    # Getting the frames and make as a queue with sequence length\n",
    "    frames_queue = deque(maxlen= SEQUENCE_LENGTH)\n",
    "    list_prediction = []\n",
    "    # we are checking the condition for video\n",
    "    while video_reader.isOpened():\n",
    "        # read the frames from video\n",
    "        ok,frame = video_reader.read()\n",
    "\n",
    "        if not ok:\n",
    "            break\n",
    "        \n",
    "        # resizing the frames        \n",
    "        resized_frame = cv2.resize(frame, (Image_height,Image_width))\n",
    "        # Normalizing the frames in the range of 0 to 1\n",
    "        normalized_frame = resized_frame / 255\n",
    "        # appending the normalize frames into frames_queue\n",
    "        frames_queue.append(normalized_frame)\n",
    "        # converting to frames_queue from float64 to float32 to support the model\n",
    "        frames_que = np.float32(frames_queue)\n",
    "\n",
    " \n",
    "        if len(frames_que) == SEQUENCE_LENGTH :\n",
    "            # Passing the all input data with dimensions like X for model\n",
    "            input_tensor= np.array(np.expand_dims(frames_que,0))\n",
    "            # getting the input indexes like y for model\n",
    "            input_index = interpreter.get_input_details()[0]['index']\n",
    "            # Both input_tensor and input_index passing through the model\n",
    "            interpreter.set_tensor(input_index, input_tensor)\n",
    "            interpreter.invoke()\n",
    "            # getting the output details \n",
    "            output_details = interpreter.get_output_details()\n",
    "            \n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            # Getting prediction probabilities of the output\n",
    "            pred = np.squeeze(output_data)\n",
    "            # Getting highest probabilities of the value as our output and index\n",
    "            predict_label = np.argmax(pred)\n",
    "            # Getting the class label based on the index\n",
    "            predicted_class_name = CLASSES_LIST[predict_label]\n",
    "\n",
    "#             end = time.time()\n",
    "#             print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$\",end-start)\n",
    "            \n",
    "            list_prediction.append(predicted_class_name)\n",
    "\n",
    "\n",
    "    video_reader.release()\n",
    "    return predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97d12e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_video_path = 'Downloads/fi034_UjEJAleb.mp4'\n",
    "result = tflite_model_prediction(input_video_path, SEQUENCE_LENGTH)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d3dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee0543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7abec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
